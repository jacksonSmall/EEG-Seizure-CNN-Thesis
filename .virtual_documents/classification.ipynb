import numpy as np
from imblearn.over_sampling import SMOTE
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, Subset
from sklearn.model_selection import KFold
from sklearn.metrics import classification_report
from tqdm import tqdm
import time


processed_A = np.load("data/processed_data/segments_A.npy")
processed_B = np.load("data/processed_data/segments_B.npy")
processed_C = np.load("data/processed_data/segments_C.npy")
processed_D = np.load("data/processed_data/segments_D.npy")
processed_E = np.load("data/processed_data/segments_E.npy")

print(processed_A.shape)
print(processed_B.shape)
print(processed_C.shape)
print(processed_D.shape)
print(processed_E.shape)





# A vs E

# Combine data
X = np.concatenate([processed_A, processed_E], axis=0)

# Labels: A=0, E=1
y = np.concatenate([np.zeros(processed_A.shape[0]), np.ones(processed_E.shape[0])])

print("X shape:", X.shape)
print("y shape:", y.shape)

# Convert (num_segments, window_size) → (num_segments, window_size)
X_flat = X.reshape(X.shape[0], -1)
print("Flattened X shape:", X_flat.shape)


smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X_flat, y)

print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)



device = "cuda" if torch.cuda.is_available() else "cpu"

# Convert to tensors (keep on CPU for now)
X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # (n_samples, 1, 256)
y_tensor = torch.tensor(y, dtype=torch.long)

# --- CNN model ---
class cnn_model(nn.Module):
    def __init__(self):
        super(cnn_model, self).__init__()
        self.conv1 = nn.Conv1d(1, 16, 3, padding=1)
        self.conv2 = nn.Conv1d(16, 32, 3, padding=1)
        self.pool = nn.MaxPool1d(2)
        # Optional: you can add dropout if overfitting
        # self.dropout = nn.Dropout(0.3)
        self.fc1 = nn.Linear(32 * 64, 128)  # dynamically adjust if input length changes
        self.fc2 = nn.Linear(128, 2)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        # flatten all except batch dim
        x = x.view(x.size(0), -1)
        # x = self.dropout(F.relu(self.fc1(x)))  # optional dropout
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# --- 10-Fold Cross-Validation ---
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = cnn_model().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Training
    for epoch in range(10):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # move batch to device
            
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # Batch accuracy
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/10]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # Evaluation
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = accuracy_score(y_true, y_pred) * 100
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['A', 'E'], digits=4))

total_end = time.time()
print(f"\nA vs E - Average 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")






# B vs E

# Combine data
X = np.concatenate([processed_B, processed_E], axis=0)

# Labels: B=0, E=1
y = np.concatenate([np.zeros(processed_B.shape[0]), np.ones(processed_E.shape[0])])

print("X shape:", X.shape)
print("y shape:", y.shape)

# Convert (num_segments, window_size) → (num_segments, window_size)
X_flat = X.reshape(X.shape[0], -1)
print("Flattened X shape:", X_flat.shape)


smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)

print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)


# k 10 fold cv
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()  # start

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()  # fold seconds
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = cnn_model().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # tqdm bar
    for epoch in range(15):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # eval
    model.eval()
    y_true = []
    y_pred = []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['A', 'E'], digits=4))

total_end = time.time()
print(f"\nB vs E Average 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")








# Combine EEG data: processed_A, processed_B, processed_E (numpy arrays)
X = np.concatenate([processed_A, processed_B, processed_E], axis=0)
# Labels: A + B = 0, E = 1
y = np.concatenate([np.zeros(processed_A.shape[0] + processed_B.shape[0]),
                    np.ones(processed_E.shape[0])])

print("Original X shape:", X.shape)
print("Original y shape:", y.shape)
print("Class distribution:", np.unique(y, return_counts=True))

# Flatten for SMOTE
X_flat = X.reshape(X.shape[0], -1)
smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)
print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)

# Reshape for CNN: [n_samples, 1, 256]
X_res = X_res.reshape(-1, 1, X.shape[1])

# Convert to tensors
device = "cuda" if torch.cuda.is_available() else "cpu"
X_tensor = torch.tensor(X_res, dtype=torch.float32).to(device)
y_tensor = torch.tensor(y_res, dtype=torch.long).to(device)

# kf cv
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = EEG_CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Train
    for epoch in range(10):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc bar
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # evalute
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['A+B', 'E'], digits=4))

total_end = time.time()
print(f"\n(A,B) vs E verage 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")






# combine A,B,C vs E
X = np.concatenate([processed_A, processed_B, processed_C, processed_E], axis=0)

# Labels: A + B + C = 0, E = 1
y = np.concatenate([np.zeros(processed_A.shape[0] + processed_B.shape[0] + processed_C.shape[0]),
                    np.ones(processed_E.shape[0])])


print("Original X shape:", X.shape)
print("Original y shape:", y.shape)
print("Class distribution:", np.unique(y, return_counts=True))

# Flatten for SMOTE
X_flat = X.reshape(X.shape[0], -1)
smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)
print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)

# Reshape for CNN: [n_samples, 1, 256]
X_res = X_res.reshape(-1, 1, X.shape[1])

# tensors
device = "cuda" if torch.cuda.is_available() else "cpu"
X_tensor = torch.tensor(X_res, dtype=torch.float32).to(device)
y_tensor = torch.tensor(y_res, dtype=torch.long).to(device)

# kf cv 10
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = EEG_CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Train
    for epoch in range(10):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc bar
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # eval
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['A+B+C', 'E'], digits=4))

total_end = time.time()
print(f"\n(A,B,C) vs E Average 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")






# combine A,B,C,D vs E
X = np.concatenate([processed_A, processed_B, processed_C, processed_D, processed_E], axis=0)

# Labels: A + B + C + D = 0, E = 1
# Labels: A+B+C+D = 0, E = 1
y = np.concatenate([np.zeros(processed_A.shape[0] + processed_B.shape[0] + processed_C.shape[0] + processed_D.shape[0]),np.ones(processed_E.shape[0])])



print("Original X shape:", X.shape)
print("Original y shape:", y.shape)
print("Class distribution:", np.unique(y, return_counts=True))

# Flatten for SMOTE
X_flat = X.reshape(X.shape[0], -1)
smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)
print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)

# Reshape for CNN: [n_samples, 1, 256]
X_res = X_res.reshape(-1, 1, X.shape[1])

# Convert to tensors
device = "cuda" if torch.cuda.is_available() else "cpu"
X_tensor = torch.tensor(X_res, dtype=torch.float32).to(device)
y_tensor = torch.tensor(y_res, dtype=torch.long).to(device)

# kf cv 10
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = EEG_CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # Train
    for epoch in range(10):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc bar
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # eval
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['A+B+C+D', 'E'], digits=4))

total_end = time.time()
print(f"\n(A,B,C,D) vs E Average 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")






# combine C vs E
X = np.concatenate([processed_C, processed_E], axis=0)

# Labels: C, E = 1
y = np.concatenate([np.zeros(processed_C.shape[0]),
                    np.ones(processed_E.shape[0])])


print("Original X shape:", X.shape)
print("Original y shape:", y.shape)
print("Class distribution:", np.unique(y, return_counts=True))

# Flatten for SMOTE
X_flat = X.reshape(X.shape[0], -1)
smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)
print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)

# Reshape for CNN: [n_samples, 1, 256]
X_res = X_res.reshape(-1, 1, X.shape[1])

# Convert to tensors
device = "cuda" if torch.cuda.is_available() else "cpu"
X_tensor = torch.tensor(X_res, dtype=torch.float32).to(device)
y_tensor = torch.tensor(y_res, dtype=torch.long).to(device)


# kf cv
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = EEG_CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # train
    for epoch in range(15):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc bar
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # eval
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['C', 'E'], digits=4))

total_end = time.time()
print(f"\nAverage 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")






# combine D vs E
X = np.concatenate([processed_D, processed_E], axis=0)

# Labels: D, E = 1
y = np.concatenate([np.zeros(processed_D.shape[0]),
                    np.ones(processed_E.shape[0])])


print("Original X shape:", X.shape)
print("Original y shape:", y.shape)
print("Class distribution:", np.unique(y, return_counts=True))

# Flatten for SMOTE
X_flat = X.reshape(X.shape[0], -1)
smote = SMOTE(random_state=21)
X_res, y_res = smote.fit_resample(X_flat, y)
print("After SMOTE - X:", X_res.shape, "y:", y_res.shape)

# Reshape for CNN: [n_samples, 1, 256]
X_res = X_res.reshape(-1, 1, X.shape[1])

# Convert to tensors
device = "cuda" if torch.cuda.is_available() else "cpu"
X_tensor = torch.tensor(X_res, dtype=torch.float32).to(device)
y_tensor = torch.tensor(y_res, dtype=torch.long).to(device)


# kf cv
kf = KFold(n_splits=10, shuffle=True, random_state=4000)
fold_accuracies = []

total_start = time.time()

for fold, (train_idx, test_idx) in enumerate(kf.split(X_tensor)):
    print(f"\nFold {fold+1}")
    fold_start = time.time()
    
    train_dataset = Subset(TensorDataset(X_tensor, y_tensor), train_idx)
    test_dataset = Subset(TensorDataset(X_tensor, y_tensor), test_idx)
    
    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=64)
    
    model = EEG_CNN().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # train
    for epoch in range(15):
        model.train()
        loop = tqdm(train_loader, leave=False)
        for X_batch, y_batch in loop:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            
            # acc bar
            _, preds = torch.max(outputs, 1)
            acc = (preds == y_batch).float().mean().item() * 100
            loop.set_description(f"Epoch [{epoch+1}/15]")
            loop.set_postfix(loss=f"{loss.item():.4f}", acc=f"{acc:.2f}%")
    
    # eval
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(y_batch.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
    
    acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))
    fold_accuracies.append(acc)
    
    fold_end = time.time()
    print(f"Fold {fold+1} Accuracy: {acc:.2f}%")
    print(f"Fold {fold+1} Time: {fold_end - fold_start:.2f} seconds")
    
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=['D', 'E'], digits=4))

total_end = time.time()
print(f"\nAverage 10-Fold Accuracy: {np.mean(fold_accuracies):.2f}% ± {np.std(fold_accuracies):.2f}%")
print(f"Total 10-Fold CV Time: {total_end - total_start:.2f} seconds")







